import { Translation } from '../../types';

export const de: Translation = {
  // App title
  appTitle: 'OpenAI LLM Metriken Dashboard',
  
  // Section headers
  basicStatistics: 'Grundlegende Statistiken',
  streamingStatistics: 'Streaming Statistiken',
  tokenUsage: 'Token Verbrauch',
  performanceMetrics: 'Leistungsmetriken',
  modelUsage: 'Modell Verwendung',
  completionAnalysis: 'Vervollst√§ndigungsanalyse',
  errorAnalysis: 'Fehleranalyse',
  
  // Metric labels
  totalCompletionRequests: 'Gesamte Vervollst√§ndigungsanfragen',
  successfulRequests: 'Erfolgreiche Anfragen',
  failedRequests: 'Fehlgeschlagene Anfragen',
  successRate: 'Erfolgsrate',
  requestsLast24h: 'Anfragen (Letzte 24h)',
  streamingRequests: 'Streaming Anfragen',
  nonStreamingRequests: 'Nicht-Streaming Anfragen',
  streamingPercentage: 'Streaming Prozentsatz',
  totalTokensUsed: 'Gesamte verwendete Tokens',
  avgTokensPerRequest: 'Durchschnittliche Tokens pro Anfrage',
  avgResponseTime: 'Durchschnittliche Antwortzeit',
  avgTokensPerSecond: 'Durchschnittliche Tokens pro Sekunde',
  lastUpdated: 'Zuletzt aktualisiert',
  
  // Button text
  refreshNow: 'Jetzt aktualisieren',
  
  // Footer
  footerText: 'OpenAI LLM Metriken Proxy',
  
  // Language names
  english: 'English',
  spanish: 'Espa√±ol',
  french: 'Fran√ßais',
  german: 'Deutsch',
  japanese: 'Êó•Êú¨Ë™û',
  
  // Notes and warnings
  tokenUsageNote: '‚ö†Ô∏è Token-Verbrauch ist nur f√ºr Nicht-Streaming-Anfragen verf√ºgbar. Streaming-Anfragen zeigen stattdessen Zeitmetriken.',
  performanceNote: 'üìä Antwortzeit umfasst sowohl Streaming- als auch Nicht-Streaming-Anfragen. Tokens pro Sekunde sind nur f√ºr Nicht-Streaming verf√ºgbar.',
  requests: 'Anfragen',
  times: 'mal',
  tokensPerSecond: 'Tokens/s',
  naStreaming: 'N/A (Streaming)',
  
  // Loading and error states
  loadingMetrics: 'Metriken werden geladen...',
  errorLoadingMetrics: 'Fehler beim Laden der Metriken:',
  noMetricsData: 'Keine Metrikdaten verf√ºgbar'
};
